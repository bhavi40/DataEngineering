# Azure End-to-End ETL Pipeline using Medallion Architecture

## ðŸ“Œ Project Overview
This project implements a complete ETL (Extract, Transform, Load) pipeline using Microsoft Azure services and the Medallion Architecture (Bronze â†’ Silver â†’ Gold).
The pipeline ingests data from GitHub and Azure SQL, performs transformations and enrichment in Azure Databricks using Cosmos DB, and finally loads curated data into Azure Synapse Analytics for analysis.

---

## Architecture Flow
![ETL Architecture Flow](https://github.com/bhavi40/DataEngineering/blob/main/Brazillian%20ecommerce/Architecture%20Diagram.png)

## 1. Workflow Steps
Created the following Azure resources within a Resource Group:
  - **Azure Data Lake Storage Gen2** - for implementing Medallion layers (Bronze, Silver, Gold)
  - **Azure Data Factory** - for pipeline orchestration
  - **Azure Databricks** - for data transformation using PySpark
  - **Azure Cosmos DB** - for data enrichment
  - **Azure Synapse Analytics** - for gold-layer reporting
---

## 2. Data Ingestion (Bronze Layer)
- Data sourced from:
   - GitHub â€“ eCommerce dataset (CSV files)
   - Azure SQL â€“ product and sales data
- Used Azure Data Factory (ADF) pipelines to ingest data from both sources into ADLS Gen2 (Bronze layer).
---

## 3. Data Transformation (Silver Layer)
- Transformation done in Azure Databricks using PySpark:
   - Data cleaning, deduplication, and formatting
   - Joins and enrichment with data from Azure Cosmos DB
   - Creation of derived columns
- Transformed data stored back in ADLS Gen2 â†’ Silver layer
---

## Data Serving (Gold Layer)
- Connected Azure Synapse Analytics to the ADLS Gen2 Silver layer.
- Created views and external tables in Synapse for the Gold layer.
- Final curated data is ready for dashboards and analytical queries.
---

## ðŸ“‚Project Structure
ðŸ“¦ Azure-ETL-Pipeline
â”‚
â”œâ”€â”€ CodeForDataIngestion/
â”‚   â”œâ”€â”€ [Scripts to send local data to Azure SQL & Cosmos DB]
â”‚   â””â”€â”€ (Acts as data source for ADF pipelines)
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ [GitHub-based source data files, e.g., ecommerce CSVs]
â”‚   â””â”€â”€ (Raw data for ingestion to ADLS Gen2 Bronze layer)
â”‚
â”œâ”€â”€ Databricks_ecommerce.ipynb
â”‚   â”œâ”€â”€ PySpark transformation logic:
â”‚   â”‚   - Cleans and enriches data
â”‚   â”‚   - Joins with Cosmos DB
â”‚   â”‚   - Writes to Silver layer in ADLS Gen2
â”‚
â”œâ”€â”€ ForEachInput.json
â”‚   â”œâ”€â”€ ADF helper file for lookup activity:
â”‚   â”‚   - Used to iterate through multiple input files dynamically
â”‚   â”‚   - Enables sequential ingestion without manual uploads
â”‚
â”œâ”€â”€ ForEachInputScript.ipynb
â”‚   â”œâ”€â”€ Python/Notebook used to generate the `ForEachInput.json` dynamically
â”‚   â”‚   - Simplifies automation of the pipelineâ€™s lookup step
â”‚
â””â”€â”€ README.md

